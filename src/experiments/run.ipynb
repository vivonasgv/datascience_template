{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sonic-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import sys\n",
    "import json\n",
    "import plotly.express as px\n",
    "sys.path.append(\"../modules\")\n",
    "from data_manager import DataManager\n",
    "\n",
    "\n",
    "import pyspark.ml\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import toniq\n",
    "import hyperopt\n",
    "from hyperopt import hp\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import tempfile\n",
    "from shutil import make_archive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-struggle",
   "metadata": {},
   "source": [
    "# Setup Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protecting-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\"verbose\": True,\n",
    "\"gt_column\": \"income\",\n",
    "\n",
    "\"hyperopt\":\n",
    "    {\n",
    "    \"metric\": \"fMeasure\",\n",
    "    \"max_evals\": 100,\n",
    "    },\n",
    "    \n",
    "\"mlflow\":\n",
    "    {\n",
    "        \"experiment_name\": \"EXP2\",\n",
    "        \"tags\": {\"version\": \"0.1.0\"} \n",
    "    },\n",
    "\"data\":\n",
    "    {\n",
    "         mode: {\"name\": f\"income_transformed_data_{mode}\", \"store\": \"feature\", \"partition\": mode}\n",
    "         for mode in [\"train\", \"test\"]\n",
    "    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-kazakhstan",
   "metadata": {},
   "source": [
    "## Initialize DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opened-defensive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_endpoint is 10.2.3.167:9000\n"
     ]
    }
   ],
   "source": [
    "dm = DataManager(provider=\"gcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-vintage",
   "metadata": {},
   "source": [
    "## Load Features from DataManager from config['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "characteristic-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs = {}\n",
    "for mode, load_table_args in config[\"data\"].items():\n",
    " dfs[mode]= dm.load_table(**load_table_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mysterious-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow_client = toniq.MlflowClient()\n",
    "\n",
    "\n",
    "def calculate_metrics( df):   \n",
    "    \"\"\"\n",
    "\n",
    "    define your own metrics to evaluate cross validation\n",
    "\n",
    "    :params:\n",
    "\n",
    "    df: dataframe containing {aprediction} and {label} columns\n",
    "\n",
    "    :returns:\n",
    "\n",
    "    confusion matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # turn gt into label\n",
    "    preds_and_labels = df.select('prediction',f.col('label').cast(t.FloatType()))\n",
    "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "\n",
    "    # confusion matrix\n",
    "\n",
    "\n",
    "    metrics_dict = dict(\n",
    "        # unweighted measures\n",
    "        tpr = metrics.truePositiveRate(label=1.0),\n",
    "        fpr = metrics.falsePositiveRate(label=1.0),\n",
    "        precision = metrics.precision(label=1.0),\n",
    "        recall = metrics.recall(label=1.0),\n",
    "        fMeasure = metrics.fMeasure(label=1.0)\n",
    "    )\n",
    "\n",
    "\n",
    "    metrics_dict= {k:round(v,3) if  k != \"confusion\" else v for k,v in metrics_dict.items()}\n",
    "\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "def train_and_eval(model_config, args):\n",
    "    \n",
    "    '''update the model config'''\n",
    "    config[\"model\"] = model_config\n",
    "    \n",
    "    '''get the model'''\n",
    "    model = getattr(getattr(pyspark.ml, model_config[\"type\"]), model_config[\"name\"])\n",
    "    model = model(**model_config[\"params\"])\n",
    "\n",
    "    \n",
    "    '''Initialize MLFLOW Experiment (If it does not exist)'''\n",
    "    # get the experiment if it exists, otherwise it will return a None\n",
    "    \n",
    "    experiment= mlflow_client.get_experiment_by_name(config[\"mlflow\"][\"experiment_name\"])\n",
    "\n",
    "    if experiment:\n",
    "        # if the experiment is not None, get the id\n",
    "        experiment_id = experiment.experiment_id\n",
    "    else:\n",
    "        # if the experiment is None, create a new exerpiment and get the experiment by name\n",
    "        experiment_id= mlflow_client.create_experiment(config[\"mlflow\"][\"experiment_name\"])\n",
    "        experiment= mlflow_client.get_experiment_by_name(config[\"mlflow\"][\"experiment_name\"])\n",
    "\n",
    "        \n",
    "    \"\"\"CREATE A NEW RUN\"\"\"\n",
    "    current_run = mlflow_client.create_run(experiment_id)\n",
    "    run_id = current_run.info.run_id\n",
    "    \n",
    "    \n",
    "    '''FIT MODEL ON TRAINING DATASET'''\n",
    "    model = model.fit(dfs[\"train\"])\n",
    "    \n",
    "    '''INFERENCE ON TRAINING AND TESTING DATASET'''\n",
    "    pred_dfs = {mode:model.transform(df) for mode, df in dfs.items()}\n",
    "    \n",
    "    \n",
    "    '''CALCULATE MODEL METRICS FOR TRAINING/TESTING SETS'''\n",
    "    metric_results= {mode: calculate_metrics(pred_df) for mode,pred_df in pred_dfs.items()}\n",
    "                \n",
    "    \n",
    "    '''Store the sample predictions into Toniq Store, and save the sample_path'''\n",
    "    config[\"metrics\"] = metric_results\n",
    "    \n",
    "    \n",
    "    '''Save Predictions as Artifact (Pandas Dictionary)'''\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.json') as fp:\n",
    "        # SAVE TEST PREDICTIONS ONLY FOR DEMO TO SAVE SPACE\n",
    "        pred_dfs[\"test\"].createOrReplaceTempView(\"pred_df\")\n",
    "        \n",
    "        #To save as a pandas array to json , we need to use a udf to conver the vector into an array\n",
    "        dm.spark.udf.register(\"TOARRAY\", lambda v: v.toArray().tolist(), t.ArrayType(t.FloatType()))\n",
    "        tmp_df = dm.spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                label,\n",
    "                TOARRAY(probability) as probability,\n",
    "                prediction\n",
    "\n",
    "            FROM pred_df\n",
    "        \"\"\").toPandas()\n",
    "        # save dataframe to json\n",
    "        tmp_df.to_json(fp.name)\n",
    "        \n",
    "        # log the predictions as an artifact\n",
    "        mlflow_client.log_artifact(run_id ,fp.name, artifact_path=\"predictions\")\n",
    "    \n",
    "    '''Save Model as an Artifact'''\n",
    "    with tempfile.TemporaryDirectory() as dp:\n",
    "        # Note the mode is 'w' so json could be dumped\n",
    "        # Note the suffix is .txt so the UI will show the file\n",
    "        \"\"\"write model to temprorary directory\"\"\"\n",
    "        model.write().overwrite().save(dp)\n",
    "        mlflow_client.log_artifact(run_id ,dp, artifact_path=\"model\")\n",
    "        \n",
    "    \n",
    "    '''Save Config of Experiment and Run '''\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.json') as fp:\n",
    "        # Note the mode is 'w' so json could be dumped\n",
    "        # Note the suffix is .txt so the UI will show the file\n",
    "        json.dump(config, fp)\n",
    "        fp.seek(0)\n",
    "        mlflow_client.log_artifact(run_id ,fp.name, artifact_path=\"config\")\n",
    "    \n",
    "    \n",
    "    '''Register Metrics in MLFLOW'''\n",
    "    for mode in metric_results.keys():\n",
    "        for metric_key, metric_val in metric_results[mode].items():\n",
    "            mlflow_client.log_metric(run_id, f\"{mode}-{metric_key}\", metric_val)\n",
    "    \n",
    "    '''Register Model Parameters in MLFLOW'''\n",
    "    for param_name, param_val in config[\"model\"][\"params\"].items():\n",
    "        mlflow_client.log_param(run_id,param_name, param_val)\n",
    "            \n",
    "    '''RETURN metric to Optimizer * (-) for minmizing negative (aka, maximize positive score)'''\n",
    "    \n",
    "    chosen_hyperopt_score = -metric_results[\"test\"][config[\"hyperopt\"][\"metric\"]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return chosen_hyperopt_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-channel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 6/100 [01:46<25:50, 16.49s/trial, best loss: -0.683]"
     ]
    }
   ],
   "source": [
    "\n",
    "# define a search space\n",
    "\n",
    "'''\n",
    "{\n",
    "    'type': 'classification',\n",
    "    'name': 'RandomForestClassifier',    \n",
    "    'params': dict(maxDepth=10, maxBins=49, minInstancesPerNode=2, numTrees= 10)\n",
    "    }\n",
    "'''\n",
    "\n",
    "\n",
    "space = hp.choice('model',      \n",
    "    [\n",
    "        {\n",
    "            'type': 'classification',\n",
    "            'name': 'RandomForestClassifier',    \n",
    "            'params': {\n",
    "                       \"maxDepth\":hp.quniform(\"maxDepth\",5,10,1),\n",
    "                       \"maxBins\":hp.quniform(\"maxBins\", 45,60,1),\n",
    "                       \"minInstancesPerNode\":hp.quniform(\"minInstancesPerNode\", 40,60,1),\n",
    "                       \"numTrees\":hp.quniform(\"numTrees\", 40,60,1)\n",
    "                      }\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'type': 'classification',\n",
    "            'name': 'GBTClassifier',   \n",
    "            \n",
    "            \n",
    "            'params': {\n",
    "                       \"maxDepth\":hp.quniform(\"maxDepth_GBT\",5,10,1),\n",
    "                       \"maxBins\":hp.quniform(\"maxBins_GBT\", 45,60,1),\n",
    "                       \"minInstancesPerNode\":hp.quniform(\"minInstancesPerNode_GBT\", 40,60,1),\n",
    "                      }\n",
    "        }\n",
    "    ])\n",
    "\n",
    "# minimize the objective over the space\n",
    "from hyperopt import fmin, tpe\n",
    "best = fmin(partial(train_and_eval, args=config), space, algo=tpe.suggest, max_evals=config[\"hyperopt\"][\"max_evals\"])\n",
    "\n",
    "print(best)\n",
    "print(\"Best \", hyperopt.space_eval(space, best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /tmp/tmpnwcp94sq.pth/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-treat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toniq-python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
